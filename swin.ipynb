{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGTJ9oG7aIFQ",
        "outputId": "12cf6d76-f446-44e6-fdcc-feedb7d73256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "from einops import rearrange, repeat"
      ],
      "metadata": {
        "id": "uJ1YUDRxafhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as f"
      ],
      "metadata": {
        "id": "GfLYRCGkavQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging_Conv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, downscaling_factor):\n",
        "    super().__init__()\n",
        "    self.patch_merge_conv = nn.Conv2d(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size=downscaling_factor,\n",
        "        stride=downscaling_factor,\n",
        "        padding=0\n",
        "    )\n",
        "  def forward(self,x): # x - (B, C(last stage), H, W)\n",
        "      x = self.patch_merge_conv(x) # x - (B, C, H, W)\n",
        "      return x.permute(0,2,3,1) # x - (B, H, W, C)"
      ],
      "metadata": {
        "id": "dmgnscZtsvCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
        "        super().__init__()\n",
        "        self.downscaling_factor = downscaling_factor\n",
        "        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n",
        "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
        "        x = self.patch_merge(x) # x - (1,48,3136) | (1,384,784) | (1,768,196) | (1,1536,49) : (b,df*df*oldC, L=new_h*new_w)\n",
        "        x = x.view(b, -1, new_h, new_w) # x - (b,df*df*oldC,new_h,new_w)\n",
        "        x = x.permute(0, 2, 3, 1) # x - (b,new_h,new_w,df*df*oldC)\n",
        "        x = self.linear(x) # x - (b,new_h,new_w,C)\n",
        "        return x"
      ],
      "metadata": {
        "id": "IuWukAju8L2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dmx = torch.randn(1,3,224,224)\n",
        "pm = PatchMerging(3, 96, 4)\n",
        "dmx = pm(dmx)\n",
        "dmx2 = torch.randn(1,96,56,56)\n",
        "pm = PatchMerging(96, 192, 2)\n",
        "dmx2 = pm(dmx2)\n",
        "dmx.shape, dmx2.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqT2rJ2gCTc4",
        "outputId": "9650070a-92fa-4ea1-8cad-197148312284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 56, 56, 96]), torch.Size([1, 28, 28, 192]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x"
      ],
      "metadata": {
        "id": "g1_iI6KPyZVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LN(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim) # dim = C\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs) # Pre Norm for V1\n",
        "        # return self.norm(self.fn(x), **kwargs) # Post Norm for V2"
      ],
      "metadata": {
        "id": "VIoONrTcyllX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  # dim = C = 96 | 192 | 384 | 768\n",
        "  # mlp_dim = C*4\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "Z7ZJ6RK5ysRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try ln\n",
        "torch.manual_seed(10)\n",
        "B,H,W,C = 1, 2, 2, 3\n",
        "input = torch.randn(B,H,W,C) * 10\n",
        "input[0,0,0,0] = 1\n",
        "input[0,0,0,1] = 2\n",
        "input[0,0,0,2] = 3\n",
        "\n",
        "print(input)\n",
        "print(nn.LayerNorm(C)(input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8825UH4NyzRT",
        "outputId": "3fe9913d-e5d6-4f85-95b8-37a85bc1d229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[  1.0000,   2.0000,   3.0000],\n",
            "          [-12.2769,   9.1983,  -3.4847]],\n",
            "\n",
            "         [[ -8.6918,  -9.5817, -11.9205],\n",
            "          [ 19.0500,  -9.3733,  -8.4647]]]])\n",
            "tensor([[[[-1.2247,  0.0000,  1.2247],\n",
            "          [-1.1445,  1.2917, -0.1471]],\n",
            "\n",
            "         [[ 1.0083,  0.3547, -1.3629],\n",
            "          [ 1.4137, -0.7413, -0.6724]]]], grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask(window_size, displacement, upper_lower, left_right):\n",
        "    mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
        "\n",
        "    # masking : exp(-inf) gives 0\n",
        "    if upper_lower: # last row mask\n",
        "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf') # down left section mask\n",
        "        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf') # up right section mask\n",
        "\n",
        "    if left_right: # last column mask\n",
        "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
        "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
        "        mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
        "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "aTRqmyOvWheY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_mask(3, 1, False, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X68aY82f1mq",
        "outputId": "1f550a7d-45a0-42d5-818c-abff7a80632e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
              "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
              "        [-inf, -inf, 0., -inf, -inf, 0., -inf, -inf, 0.],\n",
              "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
              "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
              "        [-inf, -inf, 0., -inf, -inf, 0., -inf, -inf, 0.],\n",
              "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
              "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
              "        [-inf, -inf, 0., -inf, -inf, 0., -inf, -inf, 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_mask(3, 1, True, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMV8yxdogBry",
        "outputId": "a178556e-68de-4c11-ba93-65d28b1598f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
              "        [-inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0.],\n",
              "        [-inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0.],\n",
              "        [-inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.linspace(1,81,81).view(9,9)\n",
        "print(x)\n",
        "y = torch.roll(x,shifts=(-1,-1), dims=(0,1))\n",
        "print(y)\n",
        "\n",
        "# shifts = (-1,-1) : shift the window down and right\n",
        "# shifts = (1,1) : shift the window up and left (back)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iSHwmzUZ1H-",
        "outputId": "e643ec7d-b344-41cc-fbc3-b80222b5625d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n",
            "        [10., 11., 12., 13., 14., 15., 16., 17., 18.],\n",
            "        [19., 20., 21., 22., 23., 24., 25., 26., 27.],\n",
            "        [28., 29., 30., 31., 32., 33., 34., 35., 36.],\n",
            "        [37., 38., 39., 40., 41., 42., 43., 44., 45.],\n",
            "        [46., 47., 48., 49., 50., 51., 52., 53., 54.],\n",
            "        [55., 56., 57., 58., 59., 60., 61., 62., 63.],\n",
            "        [64., 65., 66., 67., 68., 69., 70., 71., 72.],\n",
            "        [73., 74., 75., 76., 77., 78., 79., 80., 81.]])\n",
            "tensor([[11., 12., 13., 14., 15., 16., 17., 18., 10.],\n",
            "        [20., 21., 22., 23., 24., 25., 26., 27., 19.],\n",
            "        [29., 30., 31., 32., 33., 34., 35., 36., 28.],\n",
            "        [38., 39., 40., 41., 42., 43., 44., 45., 37.],\n",
            "        [47., 48., 49., 50., 51., 52., 53., 54., 46.],\n",
            "        [56., 57., 58., 59., 60., 61., 62., 63., 55.],\n",
            "        [65., 66., 67., 68., 69., 70., 71., 72., 64.],\n",
            "        [74., 75., 76., 77., 78., 79., 80., 81., 73.],\n",
            "        [ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.,  1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CyclicShift(nn.Module):\n",
        "    def __init__(self, displacement):\n",
        "        super().__init__()\n",
        "        self.displacement = displacement\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2)) # to be shifted along H,W dimns\n"
      ],
      "metadata": {
        "id": "AiUrlC9kZqun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relative_distances(window_size):\n",
        "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
        "    # indices - (49,2)\n",
        "    distances = indices[None, :, :] - indices[:, None, :]\n",
        "    return distances\n",
        "    # distances - relative position of row number and column number along the height and width of window\n"
      ],
      "metadata": {
        "id": "n7S_KkDs3d_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MHWSA(nn.Module):\n",
        "   def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
        "        # dim = 96 | 192 | 384 | 768\n",
        "        # heads = num_heads = 3 | 6 | 12 | 24\n",
        "        # head_dim = 32\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        inner_dim = head_dim * heads # inner_dim - 96 | 192 | 384 | 768 (=C)\n",
        "        self.scale = head_dim ** - 0.5\n",
        "        self.window_size = window_size # 7\n",
        "        self.relative_pos_embedding = relative_pos_embedding\n",
        "        self.shifted = shifted\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False) # C*3 for Q,K,V via a single Linear layer\n",
        "        \n",
        "        if self.relative_pos_embedding:\n",
        "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
        "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1)) # relative: 13x13 \n",
        "        else:\n",
        "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2)) #absolute: 49x49\n",
        "\n",
        "        \n",
        "        if self.shifted:\n",
        "            displacement = window_size // 2\n",
        "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
        "                                                             upper_lower=True, left_right=False), requires_grad=False)\n",
        "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
        "                                                            upper_lower=False, left_right=True), requires_grad=False)\n",
        "            self.cyclic_shift = CyclicShift(-displacement)\n",
        "            self.cyclic_back_shift = CyclicShift(displacement)\n",
        "\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "  \n",
        "   def forward(self,x):\n",
        "     # x - (b,56,56,96) | (b,28,28,192) | (b,14,14,384) | (b,7,7,768) \n",
        "     if self.shifted:\n",
        "            x = self.cyclic_shift(x)\n",
        "     \n",
        "     # batch = b, image height = n_h, image width = n_w, heads = h\n",
        "     b, n_h, n_w, _, h = *x.shape, self.heads\n",
        "     nw_h = n_h // self.window_size # nw_h = num_windows in image height\n",
        "     nw_w = n_w // self.window_size # nw_w = num_windows in image width \n",
        "\n",
        "     #qkv\n",
        "     qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "\n",
        "     # d = head_dim, h = heads, w_w = window width, w_h = window height\n",
        "     q, k, v = map(\n",
        "            lambda t: rearrange(t, \n",
        "                                'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d', \n",
        "                                h=h, \n",
        "                                w_h=self.window_size, \n",
        "                                w_w=self.window_size\n",
        "                    ), qkv\n",
        "     ) \n",
        "     # q,k,v- (Batch, Number of Heads, Number of Windows, Number of Tokens in each window, Embedding Dimension of each head)\n",
        "     # q,k,v - (b,h,w,t,d), t=49, d=32, w = num_windows, t = num_tokens_in_each_window\n",
        "     # q,k,v - (b,3,64,49,32) | (b,6,16,49,32) | (b,12,4,49,32) | (b,24,1,49,32)\n",
        "\n",
        "     #dot product\n",
        "     dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
        "     # dots - (b,h,w,t,t), t=i=j=49\n",
        "\n",
        "     #pos embedding\n",
        "     \n",
        "     if self.relative_pos_embedding:\n",
        "            #relative_indices - (49,49,2), pos_embedding - (13,13) : unique possible combinations out of values of relative_indices\n",
        "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
        "     else:\n",
        "            # absolute pos embedding\n",
        "            dots += self.pos_embedding  # (b,h,w,t,t) + (t,t) = (b,h,w,t,t) \n",
        "\n",
        "     #masking\n",
        "     if self.shifted:\n",
        "       # upper_lower_mask, left_right_mask - (t,t)\n",
        "       dots[:, :, -nw_w:] += self.upper_lower_mask\n",
        "       dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n",
        "\n",
        "     \n",
        "     #softmax\n",
        "     attn = dots.softmax(dim=-1)\n",
        "     \n",
        "     #dot product with v\n",
        "     attn = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
        "     # attn - (b,h,w,t,d)\n",
        "\n",
        "     # convert to (b,n_h,n_w,C)\n",
        "     out = rearrange(attn, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
        "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
        "      \n",
        "     # project to (b,n_h,n_w,C)\n",
        "     out = self.to_out(out)\n",
        "\n",
        "     #restore the original feature sequence\n",
        "     if self.shifted:\n",
        "            out = self.cyclic_back_shift(out)\n",
        "     \n",
        "     return out\n",
        "\n",
        "            \n"
      ],
      "metadata": {
        "id": "LHJvV0PUXHCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinBlock(nn.Module):\n",
        "  def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n",
        "        # dim = C\n",
        "        # mlp_dim = C*4\n",
        "        super().__init__()\n",
        "        self.attention_block = Residual(LN(dim, MHWSA(dim=dim,\n",
        "                                                     heads=heads,\n",
        "                                                     head_dim=head_dim,\n",
        "                                                     shifted=shifted,\n",
        "                                                     window_size=window_size,\n",
        "                                                     relative_pos_embedding=relative_pos_embedding\n",
        "        )))\n",
        "        self.mlp_block = Residual(LN(dim, MLP(dim=dim, hidden_dim=mlp_dim))) \n",
        "  def forward(self, x): # x - (b,56,56,96) | (b,28,28,192) | (b,14,14,384) | (b,7,7,768)\n",
        "        x = self.attention_block(x) \n",
        "        x = self.mlp_block(x) \n",
        "        return x # x - (b,56,56,96) | (b,28,28,192) | (b,14,14,384) | (b,7,7,768)"
      ],
      "metadata": {
        "id": "GKeLZvd-u3-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Stage(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n",
        "                 relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n",
        "\n",
        "        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n",
        "                                            downscaling_factor=downscaling_factor)\n",
        "\n",
        "        # self.patch_partition = PatchMerging_Conv(in_channels=in_channels, out_channels=hidden_dimension,\n",
        "        #                                     downscaling_factor=downscaling_factor)\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(layers // 2):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "            ]))\n",
        "\n",
        "    # b = batch size, df = downscaling_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x - (b,3,224,224) | (b, 96,56,56) | (b,192,28,28) | (b,384,14,14)\n",
        "        x = self.patch_partition(x) # Patch Merging of df x df neighbouring patches\n",
        "        # x - (b,56,56,96) | (b,28,28,192) | (b,14,14,384) | (b,7,7,768)\n",
        "        for swin_block, shifted_swin_block in self.layers:\n",
        "            x = swin_block(x) \n",
        "            x = shifted_swin_block(x)\n",
        "        return x.permute(0, 3, 1, 2) # x - (b, 96,56,56) | (b,192,28,28) | (b,384,14,14) | (b,768,7,7)"
      ],
      "metadata": {
        "id": "s1WCFvq2m4zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformer(nn.Module):\n",
        "    def __init__(self, *, hidden_dim, layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
        "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Acc. to Swin T: hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24)\n",
        "        \n",
        "        # input img - (3,224,224) (3,n_h,n_w) : each in batch b\n",
        "        self.stage1 = Stage(in_channels=channels, hidden_dimension=hidden_dim, layers=layers[0],\n",
        "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        \n",
        "        # x <- b, C(hidden_dim = 96), n_h/4, n_w/4 \n",
        "        # x <- (b,96,56,56)\n",
        "        \n",
        "        self.stage2 = Stage(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, layers=layers[1],\n",
        "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        \n",
        "        # every subsequent stage does n_h/2 and n_w/2 and c*2 in x : heirarchical structure\n",
        "\n",
        "        # x <- (b,192,28,28)\n",
        "        \n",
        "        self.stage3 = Stage(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, layers=layers[2],\n",
        "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        \n",
        "        # x <- (b,384,14,14)\n",
        "        \n",
        "        self.stage4 = Stage(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[3],\n",
        "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "\n",
        "        # x <- (b,768,7,7)\n",
        "        \n",
        "        # head for classification here\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim * 8),\n",
        "            nn.Linear(hidden_dim * 8, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x1 = self.stage1(img)\n",
        "        x2 = self.stage2(x1)\n",
        "        x3 = self.stage3(x2)\n",
        "        x4 = self.stage4(x3)\n",
        "        \n",
        "        # x = x4.mean(dim=[2, 3])\n",
        "        # return self.mlp_head(x)\n",
        "        return x1,x2,x3,x4\n"
      ],
      "metadata": {
        "id": "zAapZb9VdYcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def swin_t(hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)"
      ],
      "metadata": {
        "id": "xNF3SR9M_X5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = swin_t(\n",
        "    channels=3,\n",
        "    num_classes=3,\n",
        "    head_dim=32,\n",
        "    window_size=7,\n",
        "    downscaling_factors=(4,2,2,2),\n",
        "    relative_pos_embedding=True\n",
        ")\n",
        "\n",
        "x = torch.randn(1,3,224,224)\n",
        "y1,y2,y3,y4 = net(x)\n",
        "y1.shape,y2.shape,y3.shape,y4.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5mzpXei_cxO",
        "outputId": "bab62274-b255-4d44-8962-2d32eec99463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 96, 56, 56]),\n",
              " torch.Size([1, 192, 28, 28]),\n",
              " torch.Size([1, 384, 14, 14]),\n",
              " torch.Size([1, 768, 7, 7]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ShallowFeatures_Conv():\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size=kernel_size,\n",
        "        stride=stride,\n",
        "        padding=0\n",
        "    )\n",
        "  def forward(self,x): \n",
        "      x = self.conv(x) # x - (B, C, H, W)\n",
        "      return x"
      ],
      "metadata": {
        "id": "0UkC5LZaqinI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialInvariant_Conv():\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size=kernel_size,\n",
        "        stride=stride,\n",
        "        padding=0\n",
        "    )\n",
        "  def forward(self,x): \n",
        "      x = self.conv(x) # x - (B, C, H, W)\n",
        "      return x"
      ],
      "metadata": {
        "id": "o5S2F3qElfhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddInductiveBias_Conv():\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size=kernel_size,\n",
        "        stride=stride,\n",
        "        padding=0\n",
        "    )\n",
        "  def forward(self,x): \n",
        "      x = self.conv(x) # x - (B, C, H, W)\n",
        "      return x"
      ],
      "metadata": {
        "id": "Mi_3zgqhoCXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RSTB(nn.Module):\n",
        "  def __init__(self, in_channels, hidden_dimension, layers, num_heads, head_dim, window_size,\n",
        "                 relative_pos_embedding, rstb_conv_kernel_size, rstb_conv_stride):\n",
        "    super().__init__()\n",
        "    assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n",
        "    self.layers = nn.ModuleList([])\n",
        "    for _ in range(layers // 2):\n",
        "        self.layers.append(nn.ModuleList([\n",
        "            SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                      shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "            SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                      shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "        ]))\n",
        "    \n",
        "    self.spatial_invariant_conv = SpatialInvariant_Conv(in_channels=in_channels, out_channels=hidden_dimension,kernel_size=rstb_conv_kernel_size,stride=rstb_conv_stride)\n",
        "    \n",
        "  def forward(self, x):\n",
        "      # x - (b,180,56,56) :(B,C,H,W) \n",
        "      for swin_block, shifted_swin_block in self.layers:\n",
        "          x = swin_block(x) \n",
        "          x = shifted_swin_block(x)\n",
        "      x = self.spatial_invariant_conv(x) \n",
        "      return x"
      ],
      "metadata": {
        "id": "Ex1LxkAzcpx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinIRUpscaler(nn.Module):\n",
        "  def __init__(self, *, rstb=6, stl=6, window_size=8, heads=6, channels=180, head_dim=32,\n",
        "                 rstb_conv_kernel_size=3, rstb_conv_stride=1, relative_pos_embedding=True, upscale_factor=3,\n",
        "               conv_kernel_size=3, conv_stride=1, shallow_conv_kernel_size=3, shallow_conv_stride=1):\n",
        "    # ISSUE : conv_stride = ?, conv_kernel_size = ? for multiple convolutions\n",
        "    # ISSUE: tensor shape math\n",
        "    # ISSUE: convert feature channels to image rbg channels\n",
        "    super().__init__()\n",
        "\n",
        "    self.shallow_features = ShallowFeatures_Conv(in_channels=3, out_channels=channels, kernel_size=shallow_conv_kernel_size,stride=shallow_conv_stride)\n",
        "    self.deep_extract = nn.ModuleList([])\n",
        "    \n",
        "    for _ in range(rstb):\n",
        "      self.deep_extract.append(RSTB(in_channel=channels, hidden_dimension=channels, layers=stl, num_heads=heads, head_dim=head_dim, window_size=window_size, relative_pos_embedding=relative_pos_embedding, rstb_conv_kernel_size=rstb_conv_kernel_size, rstb_conv_stride=rstb_conv_stride))\n",
        "    \n",
        "    self.deep_extract.append(AddInductiveBias_Conv(in_channels=channels, out_channels=channels,kernel_size=conv_kernel_size,stride=conv_stride))\n",
        "\n",
        "    self.subpixelconv = nn.PixelShuffle(upscale_factor) # C = C / (upscale_factor**2), H = H*upscale_factor, W = W*upscale_factor\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.shallow_features(x)\n",
        "    y = x\n",
        "    for stage in self.stages:\n",
        "      y = stage(y)\n",
        "    x = x + y # Residual\n",
        "    x = self.subpixelconv(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "PaZj8PT5agaS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}